{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337d26bb-8933-4fc9-a642-e589deafc64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c6fc12-2968-4e2d-8c3c-f5175152fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate huggingface_hub torc sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4835d401-a099-425c-af29-c8561e7b5d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e1a1623fb34985a2800f670cdbe304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68657ee3-b57c-4b6e-91ef-53c1a3536138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Meta-Llama-3-8B', revision=None, inference_mode=True, r=16, target_modules={'q_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c946831d7043308ac90761d2528d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27f8b781a4c4908b84d570799d307a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ee8704a69455aa1b400d11e49b50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174d968c38974911830676e749a727eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098ae9a5b5cd4c80ac772f06aebca148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a61b427424c22b3099b58a987099e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49eba1c30574a91b38b7d6161a42268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59b55b6c33f4c04b361b73f952258a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1a77b2c548424b96f74b445c235d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     base_model_name,\n\u001b[1;32m     14\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Load adapter with strict=False to handle mismatches\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable training if not needed\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Explicitly pass the config\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 4. Merge carefully\u001b[39;00m\n\u001b[1;32m     29\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:541\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[1;32m    534\u001b[0m         model,\n\u001b[1;32m    535\u001b[0m         config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m    539\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[1;32m    552\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    553\u001b[0m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result\u001b[38;5;241m.\u001b[39mmissing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvblora_vector_bank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[1;32m    554\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1272\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     peft_config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_adapter(adapter_name, peft_config, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage)\n\u001b[0;32m-> 1272\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:567\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m safe_load_file(filename, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m torch_load(filename, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/safetensors/torch.py:313\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mLoads a safetensors file into torch format.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    315\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(k)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: MetadataIncompleteBuffer"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# 1. Verify adapter config first\n",
    "adapter_path = \"llama3-fingpt-lora\"\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "print(f\"Adapter config: {peft_config}\")\n",
    "\n",
    "# 2. Load base model with correct architecture\n",
    "base_model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# 3. Load adapter with strict=False to handle mismatches\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    device_map=\"auto\",\n",
    "    is_trainable=False,  # Disable training if not needed\n",
    "    config=peft_config   # Explicitly pass the config\n",
    ")\n",
    "\n",
    "# 4. Merge carefully\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 5. Save merged model\n",
    "save_path = \"merged_fingpt-llama3\"\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ac92e4-296a-4338-8dea-54774163f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: adapter_model.safetensors\n",
      "✅ Found: adapter_config.json\n",
      "✔ File is valid. Contains 256 weight tensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from safetensors import safe_open\n",
    "\n",
    "adapter_path = \"llama3-fingpt-lora\"\n",
    "\n",
    "# Check if files exist\n",
    "required_files = [\"adapter_model.safetensors\", \"adapter_config.json\"]\n",
    "for file in required_files:\n",
    "    if not os.path.exists(os.path.join(adapter_path, file)):\n",
    "        print(f\"❌ Missing file: {file}\")\n",
    "    else:\n",
    "        print(f\"✅ Found: {file}\")\n",
    "\n",
    "# Validate safetensors file\n",
    "try:\n",
    "    with safe_open(f\"{adapter_path}/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "        print(f\"✔ File is valid. Contains {len(f.keys())} weight tensors\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Corrupted file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf79d66f-9d3c-422e-802c-8aac4955a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfab9dd641b406a93dcfce370f62789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7cee0214a546f9a3a9986b127f025c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9c538e629342d9b031f4e5473064f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b677aa6fa4411ca09bba5bd4c59ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged model saved\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model (adjust path if needed)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load adapter (using your verified files)\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"llama3-fingpt-lora\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Merge and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_fingpt-llama3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.save_pretrained(\"merged_fingpt-llama3\")\n",
    "\n",
    "print(\"✅ Merged model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0fe48-e7dd-4b61-8606-4b828626ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a350f01-90dc-436c-9eaa-8b43c570fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gguf\n",
      "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf) (1.24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from gguf) (6.0.1)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from gguf) (4.67.1)\n",
      "Downloading gguf-0.14.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: gguf\n",
      "Successfully installed gguf-0.14.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f9db14-b498-4edf-b59e-eccf5326b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092e9452-467b-4b4d-859e-a3dc8a79ead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 48781, done.\u001b[K\n",
      "remote: Counting objects: 100% (182/182), done.\u001b[K\n",
      "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
      "remote: Total 48781 (delta 134), reused 66 (delta 66), pack-reused 48599 (from 3)\u001b[K\n",
      "Receiving objects: 100% (48781/48781), 102.95 MiB | 22.12 MiB/s, done.\n",
      "Resolving deltas: 100% (35121/35121), done.\n",
      "Updating files: 100% (1284/1284), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd073d52-0029-4f72-866b-2c01ddc3b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/workspace/convert_lora_to_gguf.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 convert_lora_to_gguf.py merged-fingpt-llama3 --outfile fingpt-llama3.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f6726d-3234-40f9-b707-fe9a5b734930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: /root/llama.cpp: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd ~/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55cbb945-cb6f-4636-a57d-00490baa230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/workspace/convert_lora_to_gguf.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 convert_lora_to_gguf.py merged-fingpt-llama3 --outfile fingpt-llama3.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c36a7-685c-4a8e-8c3d-5c16bf2a8500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
